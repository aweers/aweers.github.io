<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on aweers blog</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on aweers blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Sep 2025 00:00:46 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DPO is SFT</title>
      <link>http://localhost:1313/posts/dpo-is-sft/</link>
      <pubDate>Sun, 07 Sep 2025 00:00:46 +0200</pubDate>
      <guid>http://localhost:1313/posts/dpo-is-sft/</guid>
      <description>&lt;h1 id=&#34;dpo-is-sft-with-relative-sequence-weighting&#34; class=&#34;relative group&#34;&gt;DPO is SFT (with relative sequence weighting) &lt;span class=&#34;absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100&#34;&gt;&lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34; style=&#34;text-decoration-line: none !important;&#34; href=&#34;#dpo-is-sft-with-relative-sequence-weighting&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;Training stages of LLMs almost certainly include a supervised fine-tuning (SFT) stage in which the model is trained with cross-entropy loss on many data samples.&#xA;Current state-of-the-art LLMs usually also employ a human preference alignment by training the model based on preference-ranked completion sequences using direct preference optimization (DPO).&lt;/p&gt;&#xA;&lt;p&gt;On first glance, those training stages and their respective loss functions might be very different, but they have a lot more in common than I thought before analyzing their exact behavior.&#xA;But let&amp;rsquo;s start with the higher-level overview:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
