






<!doctype html>
<html
  lang="en-us"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"
><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#FFFFFF" />
  
  <title>DPO is SFT &middot; aweers blog</title>
    <meta name="title" content="DPO is SFT &middot; aweers blog" />
  
  
  
  
  
  <script
    type="text/javascript"
    src="http://localhost:1313/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js"
    integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="
  ></script>
  
  
  
  
  
  
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="http://localhost:1313/css/main.bundle.min.8363fdb90097634558e4c060565c2caefbddbf21152432d1b5ca1f55adca4e75.css"
    integrity="sha256-g2P9uQCXY0VY5MBgVlwsrvvdvyEVJDLRtcofVa3KTnU="
  />
  
  
  
  
  
  
  
  <meta
    name="description"
    content="
      
        DPO is SFT (with relative sequence weighting) #Training stages of LLMs almost certainly include a supervised fine-tuning (SFT) stage in which the model is trained with cross-entropy loss on many data samples.
Current state-of-the-art LLMs usually also employ a human preference alignment by training the model based on preference-ranked completion sequences using direct preference optimization (DPO).
On first glance, those training stages and their respective loss functions might be very different, but they have a lot more in common than I thought before analyzing their exact behavior.
But let&rsquo;s start with the higher-level overview:
      
    "
  />
  
  
  
  
    <link rel="canonical" href="http://localhost:1313/posts/dpo-is-sft/" />
  
  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  
  <meta property="og:url" content="http://localhost:1313/posts/dpo-is-sft/">
  <meta property="og:site_name" content="aweers blog">
  <meta property="og:title" content="DPO is SFT">
  <meta property="og:description" content="DPO is SFT (with relative sequence weighting) #Training stages of LLMs almost certainly include a supervised fine-tuning (SFT) stage in which the model is trained with cross-entropy loss on many data samples. Current state-of-the-art LLMs usually also employ a human preference alignment by training the model based on preference-ranked completion sequences using direct preference optimization (DPO).
On first glance, those training stages and their respective loss functions might be very different, but they have a lot more in common than I thought before analyzing their exact behavior. But let’s start with the higher-level overview:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-07T00:00:46+02:00">
    <meta property="article:modified_time" content="2025-09-07T00:00:46+02:00">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="DPO is SFT">
  <meta name="twitter:description" content="DPO is SFT (with relative sequence weighting) #Training stages of LLMs almost certainly include a supervised fine-tuning (SFT) stage in which the model is trained with cross-entropy loss on many data samples. Current state-of-the-art LLMs usually also employ a human preference alignment by training the model based on preference-ranked completion sequences using direct preference optimization (DPO).
On first glance, those training stages and their respective loss functions might be very different, but they have a lot more in common than I thought before analyzing their exact behavior. But let’s start with the higher-level overview:">

  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Posts",
    "name": "DPO is SFT",
    "headline": "DPO is SFT",
    
    "abstract": "\u003ch1 id=\u0022dpo-is-sft-with-relative-sequence-weighting\u0022 class=\u0022relative group\u0022\u003eDPO is SFT (with relative sequence weighting) \u003cspan class=\u0022absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\u0022\u003e\u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022 style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#dpo-is-sft-with-relative-sequence-weighting\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\u003c\/span\u003e\u003c\/h1\u003e\u003cp\u003eTraining stages of LLMs almost certainly include a supervised fine-tuning (SFT) stage in which the model is trained with cross-entropy loss on many data samples.\nCurrent state-of-the-art LLMs usually also employ a human preference alignment by training the model based on preference-ranked completion sequences using direct preference optimization (DPO).\u003c\/p\u003e\n\u003cp\u003eOn first glance, those training stages and their respective loss functions might be very different, but they have a lot more in common than I thought before analyzing their exact behavior.\nBut let\u0026rsquo;s start with the higher-level overview:\u003c\/p\u003e",
    "inLanguage": "en-us",
    "url" : "http:\/\/localhost:1313\/posts\/dpo-is-sft\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    "copyrightYear": "2025",
    "dateCreated": "2025-09-07T00:00:46\u002b02:00",
    "datePublished": "2025-09-07T00:00:46\u002b02:00",
    
    "dateModified": "2025-09-07T00:00:46\u002b02:00",
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "2832"
  }
  </script>
    
    <script type="application/ld+json">
    {
   "@context": "https://schema.org",
   "@type": "BreadcrumbList",
   "itemListElement": [
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/",
       "name": "Aweers Blog",
       "position": 1
     },
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/posts/",
       "name": "Posts",
       "position": 2
     },
     {
       "@type": "ListItem",
       "name": "Dpo Is Sft",
       "position": 3
     }
   ]
 }
  </script>

  
  
  
  
  






  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="http://localhost:1313/lib/katex/katex.min.19095127357ed6d29fe0a63a6b000c913a89f7f1963b765dd3715e97c9852e75.css"
    integrity="sha256-GQlRJzV&#43;1tKf4KY6awAMkTqJ9/GWO3Zd03Fel8mFLnU="
  />
  
  
  <script defer src="http://localhost:1313/lib/katex/katex.min.e8d885505949f3a5f4abdd5dd0d53696bd1371ad26ffbf4f310dcd77c8cdae89.js" integrity="sha256-6NiFUFlJ86X0q91d0NU2lr0Tca0m/79PMQ3Nd8jNrok="></script>
  
  
  <script
    defer
    src="http://localhost:1313/lib/katex/auto-render.min.bb53eb953394531aae36fdd537065c4244eb8542901a3ce914601d932675b8ac.js"
    integrity="sha256-u1PrlTOUUxquNv3VNwZcQkTrhUKQGjzpFGAdkyZ1uKw="
    onload="renderMathInElement(document.body);"
  ></script>
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  


  
  

  
  
</head>
<body
    class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"
  >
    <div id="the-top" class="absolute flex self-center">
      <a
        class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content"
        ><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span
        >Skip to main content</a
      >
    </div>
    
    
      <header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden">
  <nav class="flex items-start justify-between sm:items-center">
    
    <div class="flex flex-row items-center">
      
  <a
    class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
    rel="me"
    href="/"
    >aweers blog</a
  >

    </div>
    
    
  </nav>
</header>

    
    <div class="relative flex grow flex-col">
      <main id="main-content" class="grow">
        
  <article>
    <header class="max-w-prose">
      
      <h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        DPO is SFT
      </h1>
      
        <div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
          





  
  



  

  
  
    
  

  

  

  
    
  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2025-09-07 00:00:46 &#43;0200 CEST">September 7, 2025</time><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">14 mins</span>
    

    
    
  </div>

  
  


        </div>
      
      
    </header>
    <section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row">
      
        <div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8">
          <div class="toc pe-5 lg:sticky lg:top-10 print:hidden">
            <details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5">
  <summary
    class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"
  >
    Table of Contents
  </summary>
  <div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#cross-entropy">Cross-entropy</a>
      <ul>
        <li><a href="#gradient-of-cross-entropy-loss-wrt-logits">Gradient of cross-entropy loss wrt logits</a></li>
      </ul>
    </li>
    <li><a href="#dpo">DPO</a>
      <ul>
        <li><a href="#gradient-of-dpo">Gradient of DPO</a></li>
      </ul>
    </li>
    <li><a href="#simple-example">Simple example</a>
      <ul>
        <li><a href="#supervised-fine-tuning-with-cross-entropy">Supervised fine-tuning with cross-entropy</a></li>
        <li><a href="#dpo-1">DPO</a></li>
      </ul>
    </li>
    <li><a href="#summary">Summary</a></li>
  </ul>
</nav>
  </div>
</details>

          </div>
        </div>
      
      <div class="min-h-0 min-w-0 max-w-prose grow">
        

<h1 id="dpo-is-sft-with-relative-sequence-weighting" class="relative group">DPO is SFT (with relative sequence weighting) <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#dpo-is-sft-with-relative-sequence-weighting" aria-label="Anchor">#</a></span></h1><p>Training stages of LLMs almost certainly include a supervised fine-tuning (SFT) stage in which the model is trained with cross-entropy loss on many data samples.
Current state-of-the-art LLMs usually also employ a human preference alignment by training the model based on preference-ranked completion sequences using direct preference optimization (DPO).</p>
<p>On first glance, those training stages and their respective loss functions might be very different, but they have a lot more in common than I thought before analyzing their exact behavior.
But let&rsquo;s start with the higher-level overview:</p>
<blockquote>
<p><strong>SFT with cross-entropy loss</strong> (<em>the classic, the powerhouse</em>)
#linebreak()
What would deep learning be without cross-entropy loss? It&rsquo;s not only a multi-time-proven, well-working loss function that can be efficiently and stably implemented, but it is also deeply rooted in a probabilistic theoretical framework.
I won&rsquo;t start from the concept of entropy here, even though it is quite interesting (see <a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/" target="_blank" rel="noreferrer">this article</a> for more information). <br>
In LLM training, SFT is usually employed after the pretraining (which <em>also</em> uses cross-entropy loss) to <em>teach</em> the model chat template, instruction following, and some general concepts and behaviors. In this article we simplify the dataset to just contain single-turn prompt-answer pairs ($\mathcal{D}_\text{CE} = \{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \dots, (x^{(n)}, y^{(n)})\}$), where $x$ is a prompt (e.g. question or instruction) and $y$ is the expected answer. During training, we let the model predict every token of the answer based on the prompt and all previous answer tokens: $\hat{y}_{t + 1} = \text{LLM}(x; y_{< t})$.<br>
The cross-entropy loss together with the optimizer then <em>pushes</em> the predicted probability distribution over all tokens of the vocabulary towards the one-hot encoded ground-truth.</p></blockquote>
<blockquote>
<p><strong>Alignment with Direct Preference Optimization (DPO)</strong> (<em>the new, the pure</em>)<br>
After <em>teaching</em> the model basic templates, instruction following, and some general behaviors, an additional alignment stage is necessary to further improve the quality of predictions. DPO (as well as other alignment methods) enables the <em>teaching</em> of hard-to-describe behaviors, output formats, language usage, and structure. I definitely recommend <a href="https://arxiv.org/abs/2305.18290" target="_blank" rel="noreferrer">the paper</a>, since it is well-written and does a good job motivating the method and explaining it in a simple way.<br>
DPO requires an already pre-trained and fine-tuned model, since the first step is to sample multiple completions for a set of prompts. In this article we assume just two completions, which are ranked and labelled as <em>preferred</em> (commonly referred to as <em>winning</em>) and <em>dispreferred</em> (<em>losing</em>): $\mathcal{D}_\text{DPO} = \{(x^{(1)}, y_w^{(1)}, y_l^{(1)}), (x^{(2)}, y_w^{(2)}, y_l^{(2)}), \dots, (x^{(n)}, y_w^{(n)}, y_l^{(n)})\}$. During training, both <em>sequence probabilities</em> are computed, and the objective of the loss is to increase the likelihood of the preferred completion while simultaneously decreasing the probability of the dispreferred one.</p></blockquote>
<blockquote>
<p><strong>Sequence probabilities</strong> is a term frequently used in this summary, so lets quickly define it: A sequence probability  is the product of the token probabilities, i.e., $\pi(y |x) = \prod_i p(y_i |x, y_{< i})$.</p></blockquote>
<p>Both loss functions have different objectives, different settings in which they are employed, and require different datasets, so why should they be further compared, and how are they connected?
Well, on closer look, they share also quite a few things, and it becomes hard to really tell the difference. Let me slightly reformulate their descriptions:</p>
<ul>
<li><strong>Cross-entropy</strong> lets the model predict next tokens based on some context (prompt + previous completion tokens) and tries to move the predicted probability distribution towards the ground-truth completion.</li>
<li><strong>DPO</strong> lets the model estimate probabilities of possible completions and tries to increase the probability of the preferred completion and reduce the probability of the dispreferred completion.</li>
</ul>
<p>When I started to dive deeper into the details of DPO, I had an image of both methods close to those descriptions, and it was hard for me to pinpoint the exact difference between them.</p>
<p>So here is a summary of the deep dive with calculated gradients and a toy example to empirically show the concrete difference.</p>
<h2 id="cross-entropy" class="relative group">Cross-entropy <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#cross-entropy" aria-label="Anchor">#</a></span></h2><p>Well, cross-entropy is probably known, but here is the formula again (for a single data sample $(x, y) \in \mathcal{D}$):
</p>
$$
 \begin{aligned}
 \mathcal{L}_\text{ce} & = - \frac{1}{N} \sum_{i = 1}^N \sum_{j = 0}^V y_{i, j} \log(\hat{y}_{i, j}) \\
              & = - \frac{1}{N} \sum_{i = 1}^N \log(\hat{y}_{i, t_i})
\end{aligned}
$$<p>
where</p>
<ul>
<li>$N$ is the sequence length (number of token)</li>
<li>$V$ is the vocabulary size</li>
<li>$y_{i,j}$ is the target probability token at position $i$ being token $j$ (usually either $0$ or $1$)</li>
<li>$\hat{y}_{i, j}$ is the predicted probability that token at position $i$ is token $j$</li>
<li>and $t_i$ is the true token at position $i$</li>
</ul>
<h3 id="gradient-of-cross-entropy-loss-wrt-logits" class="relative group">Gradient of cross-entropy loss wrt logits <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#gradient-of-cross-entropy-loss-wrt-logits" aria-label="Anchor">#</a></span></h3><p>To get a better understanding of how both loss functions work, what they optimize and how they differ, let&rsquo;s have a look at the gradients with respect to the logits. The gradients will let us understand how a gradient descent algorithm would act based on the loss.</p>
<p>So let&rsquo;s derive the loss function with respect to the logits $z$ with $\hat{y}_k = \pi_\theta (k) = \text{softmax}(z_k) = \frac{e^{z_k}}{\sum_{i = 1}^{|\mathcal{V} |} e^{z_i}}$:
</p>
$$
\begin{aligned}
  \frac{\partial \mathcal{L}}{\partial z_k} & = \hat{y}_k - y_k \\
    \frac{\partial \mathcal{L}}{\partial z} & = \hat{y} - \boldsymbol{e}_y
\end{aligned}
$$<p>The gradient is very simple, but that does not mean it is less powerful. For the ground-truth token the gradient becomes $\hat{y}_k - 1$, so it will be negative.
The update step of a gradient descent optimizer (something like $\eta_{t + 1} = \eta_t - \alpha \frac{\partial \mathcal{L}}{\partial \eta}$) would then increase the score of this logit, which further reinforces the correct choice.
Similarly, for the incorrect tokens, we get a gradient of just $\hat{y}_k$, so a gradient descent optimizer would decrease those logits.
Note that the steepness of the gradient correlates with the predicted scores, so correct tokens with an already high score receive a smaller update than completely incorrect predicted tokens.</p>
<h2 id="dpo" class="relative group">DPO <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#dpo" aria-label="Anchor">#</a></span></h2><p>Direct preference optimization (DPO) requires for each data sample $x$ a preferred answer $y_w$ and a dispreferred answer $y_l$. Usually, they are generated using the model currently in training and ranked by humans or stronger models.
The loss is calculated with this formula:
</p>
$$
  \mathcal{L}_\text{DPO} = - \mathbb{E}_{x, y_w, y_l \in \mathcal{D}} \left[ \log \sigma \left(\beta \log \frac{\pi_\theta (y_w | x)}{\pi_\text{ref} (y_w | x)} - \beta \log \frac{\pi_\theta (y_l | x)}{\pi_\text{ref} (y_l | x)} \right) \right]
$$<p>
We can define some functions to make it better understandable and simplify the next steps:
</p>
$$
\begin{align}
  \hat{r} (y | x) = \beta \log \frac{\pi_\theta (y | x)}{\pi_\text{ref} (y | x)} \\
  \Delta = \beta \log \frac{\pi_\theta (y_w | x)}{\pi_\text{ref} (y_w | x)} - \beta \log \frac{\pi_\theta (y_l | x)}{\pi_\text{ref} (y_l | x)} = \hat{r} (y_w | x) - \hat{r} (y_l | x)
\end{align}
$$<p>
With that the loss function for a single data sample $(x, y_w, y_l) \in \mathcal{D}$ simplifies to
</p>
$$\mathcal{L}_{DPO} = - \log \sigma (\Delta)$$<h3 id="gradient-of-dpo" class="relative group">Gradient of DPO <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#gradient-of-dpo" aria-label="Anchor">#</a></span></h3><p>We can now derive the DPO loss for one sample wrt the logits:</p>
$$
  \begin{aligned}
\nabla_\theta \mathcal{L}_\text{DPO} &= - \frac{1}{\sigma (\Delta)} \sigma(\Delta) (1 - \sigma(\Delta)) \nabla_\theta \Delta \\
  &= \sigma(-\Delta) (\nabla_\theta \hat{r}(y_w | x) - \nabla_\theta \hat{r}(y_l | x)) \\
  & = \sigma(-\Delta) \beta (\nabla_\theta \log \pi_\theta (y_w | x) - \nabla_\theta \log \pi_\theta (y_l | x))
\end{aligned}
$$<p>
We can replace $\pi_{\theta}(y | x)$ with $\hat{y}$ and derive the gradient wrt to the logits. By now we can already notice the similarity to cross-entropy loss:
</p>
$$
\begin{aligned}
\frac{\partial \mathcal{L}_\text{DPO}}{\partial z} &= \beta \sigma(-\Delta) (\hat{y}_w - e_{y_w} - (\hat{y}_l - e_{y_l}))
\end{aligned}
$$<p>
Here we see the reason for the title of this summary: The gradient of DPO for the preferred sample is the same as for cross-entropy up to a scaling factor. For the dispreferred sample we just have an inverted gradient.</p>
<p>So basically the only, but crucial difference between SFT with cross-entropy and DPO is the relative difference between the normalized sequence probabilities of the preferred and the dispreferred sample. Importantly, not token probabilities are used here, but whole normalized sequence probabilities, so the factor is the same for all token in a sequence. Normalized, since we look at $\log \frac{\pi_\theta (y | x)}{\pi_\text{ref} (y | x)}$ instead of $\log \pi_\theta (y | x)$, but the general intuition is similar.</p>
<p>Lets have a look at some examples for the weighting factor before we continue with a full example:</p>
<ul>
<li><em>The preferred answer is already strongly preferred by the model</em>: In cases where the normalized probability of the preferred answer is already higher than that of the dispreferred answer $\hat{r} (y_w | x) \gg \hat{r} (y_l | x)$ we end up with a very small weighting: The large, positive value of $\Delta$ leads to a value close to $0$ because of the sigmoid in the weighting. So we receive only a <strong>small gradient</strong> for both the preferred and dispreferred sequences.</li>
<li><em>Both answers have a similar probability</em>: If the model has no clear preference we get roughly the same sequence probabilities $\hat{r} (y_w | x) \approx \hat{r} (y_l | x)$ and the scaling factor becomes roughly $\beta/2$.</li>
<li><em>The dispreferred answer is strongly preferred by the model</em>: In cases where the preference is inverted to our expectation with $\hat{r} (y_w | x) \ll \hat{r} (y_l | x)$, we get a high scaling factor close to $beta$.</li>
</ul>
<p>Another interesting observation from the gradient is that due to the symmetry for preferred and dispreferred samples these parts cancel for shared prefixes. This means that the model&rsquo;s parameters are only updated based on the parts of the sequences that actually differ.</p>
<h2 id="simple-example" class="relative group">Simple example <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#simple-example" aria-label="Anchor">#</a></span></h2><p>Assumptions:</p>
<ul>
<li>Vocabulary $\mathcal{V} = \{A, B, C\}$</li>
<li>Dataset $\mathcal{D} = [(x, A, B), (x, C, B)]$ where $y_w = (A, B)$ was a preferred answer compared to $y_l = (C, B)$</li>
<li>Model conditional probabilities under current policy $\pi_\theta$:
<ul>
<li>Predicting first token: $\pi_\theta (\hat{y}_1 | x) = \begin{pmatrix}0.5 \\ 0.1 \\ 0.4\end{pmatrix}$ for $\begin{pmatrix}A \\ B \\ C\end{pmatrix}$</li>
<li>Predicting second token after $\hat{y}_1=A$: $\pi_\theta (\hat{y}_2 | [x, A]) = \begin{pmatrix}0.3 \\ 0.6 \\ 0.1\end{pmatrix}$</li>
<li>Predicting second token after $\hat{y}_1=C$: $\pi_\theta (\hat{y}_2 | [x, C]) = \begin{pmatrix}0.2 \\ 0.3 \\ 0.5\end{pmatrix}$</li>
<li>Note that especially when generating responses for DPO not the token with the highest probability is chosen. Instead the next token is sampled according to the output distribution.</li>
</ul>
</li>
</ul>
<h3 id="supervised-fine-tuning-with-cross-entropy" class="relative group">Supervised fine-tuning with cross-entropy <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#supervised-fine-tuning-with-cross-entropy" aria-label="Anchor">#</a></span></h3><p>Let&rsquo;s start with the easier part, cross-entropy loss on the preferred answer. Therefore we simplify the dataset to $\mathcal{D}_\text{SFT} = [(x, A, B)]$ (since that is the preferred answer).
Given the cross-entropy loss </p>
$$\mathcal{L}_\text{CE} = - \frac{1}{N} \sum_{i = 1}^N \log(\hat{y}_{i, t_i})$$<p> we can compute the loss for our small toy example:
</p>
$$\begin{aligned}
\mathcal{L}_\text{CE} &= - \frac{1}{2} \sum_{i = 1}^2 \log(\hat{y}_{i, t_i}) \\
\mathcal{L}_\text{CE} &= - (\log (0.5) + \log (0.6)) \approx - (-0.69 + - 0.51) = 1.2
\end{aligned}$$<p>So our loss is $1.2$, but way more interesting are the gradients. So let&rsquo;s derive the loss function with respect to the logits $z$ with $\pi_\theta (i) = \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j = 1}^{|\mathcal{V} |} e^{z_j}}$:
</p>
$$\frac{\partial \mathcal{L}}{\partial z_k} = \hat{y}_k - y_k$$<p>
The full derivation is left as an exercise to the reader ;)
That let us calculate the per token gradient of the short sequence:</p>
<ul>
<li>for the first predicted token: $\frac{\partial \mathcal{L}}{\partial z} =\begin{pmatrix}0.5 - 1 \\ 0.1 - 0 \\ 0.4 - 0\end{pmatrix} = \begin{pmatrix} - 0.5 \\ 0.1 \\ 0.4 \end{pmatrix}$</li>
<li>for the second predicted token: $\frac{\partial \mathcal{L}}{\partial z} = \begin{pmatrix}0.3 - 0 \\ 0.6 - 1 \\ 0.1 - 0 \end{pmatrix} = \begin{pmatrix} 0.3 \\ - 0.4 \\ 0.1 \end{pmatrix}$</li>
</ul>
<p>Using a gradient descent optimizer negative gradients lead to an increase, while for positive gradients the logits of the tokens will be decreased.
Concretely for our example that leads to the expected behavior: The logits of the ground truth token at each step are increased, proportional to their current score (steeper gradient for worse predictions). Similarly, due to the softmax function, we have implicit negative samples, so all logits of not top-1 tokens are decreased, again with a steepness based on their current score.</p>
<h3 id="dpo-1" class="relative group">DPO <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#dpo-1" aria-label="Anchor">#</a></span></h3><p>DPO has two additional things we need to consider:</p>
<ul>
<li>Hyperparameter $\beta$: We set $\beta=1$, since our predictions are not overconfident (we will see the effect of $\beta$ in a second).</li>
<li>Reference policy $\pi_\text{ref}$: Usually that is the model before the DPO training starts, but in our toy example we assume it to be the uniform distribution: $\pi_\text{ref} = \begin{pmatrix} \frac{1}{3} \\ \frac{1}{3} \\ \frac{1}{3} \end{pmatrix}$</li>
</ul>
<p>Now we can calculate the DPO loss for our example:
</p>
$$
  \begin{aligned}
\mathcal{L}_\text{DPO} &= - \log \sigma \left(\beta \log \frac{\pi_\theta (y_{w_i} | x)}{\pi_\text{ref} (y_{w_i} | x)} - \beta \log \frac{\pi_\theta (y_{l_i} | x)}{\pi_\text{ref} (y_{l_i} | x)} \right) \\
  &= - \log \sigma  (\sum_{i = 1}^2 \log \pi_\theta (y_{w_i} | x, y_{w_{< i}}) - \sum_{i = 1}^2 \log \pi_\text{ref} (y_{w_i} | x, y_{w_{< i}})   \\ & \quad - \sum_{i = 1}^2 \log \pi_\theta (y_{l_i} | x, y_{l_{< i}}) - \sum_{i = 1}^2 \log \pi_\text{ref} (y_{l_i} | x, y_{l_{< i}}) ) \\
  &= - \log \sigma  ((-0.693 - 0.511) - 2 \dot (-1.099)   - (-0.916 - 1.204) - 2 \dot (-1.099)) \\
  &= - \log \sigma  (0.994   - 0.078 ) \\
  &= - \log \sigma  (0.916) \\
  &= - \log 0.714 \\
  &= - 0.337
\end{aligned}
$$<p>
Here we can see the effect of $\beta$. If its a small value we always stay close to values of $0.5$ from the sigmoid functions.
Note that in practice LLMs are likely to be overconfident, so their output values tend to be either close to zero or close to one. With a small value for $\beta$ we reduce those extreme values somewhat.</p>
<p>Next, lets have a look at the gradient. Recall the formula of the gradient of DPO as:
</p>
$$
  \begin{aligned}
\frac{\partial \mathcal{L}_\text{DPO}}{\partial z} &= \beta \sigma(-\Delta) (\hat{y}_w - e_{y_w} - (\hat{y}_l - e_{y_l}))
\end{aligned}
$$<p>Continuing with the toy example:</p>
<ul>
<li>that brings us to the sequence reward for the preferred sequence:
$$
  \begin{aligned}
r_\theta (y_w | x) &= \beta \log \frac{\pi_\theta (y_w | x)}{\pi_\text{ref} (y_w | x)} = \beta \sum_{t = 1}^T \log \frac{\pi_\theta (y_{w, t} | x, y_{w, < t})}{\pi_\text{ref} (y_{w, t} | x, y_{w, < t})} \\
  &=  \log(0.5) + \log(0.6) - 2 \log \left(\frac{1}{3} \right) \approx 0.99
\end{aligned}
$$</li>
<li>and dispreferred sequence:
$$
  \begin{aligned}
r_\theta (y_l | x) &= \beta \log \frac{\pi_\theta (y_l | x)}{\pi_\text{ref} (y_l | x)} = \beta \sum_{t = 1}^T \log \frac{\pi_\theta (y_{l, t} | x, y_{l, < t})}{\pi_\text{ref} (y_{l, t} | x, y_{l, < t})} \\
  &= \log(0.4) + \log(0.3) - 2 \log \left(\frac{1}{3} \right)  \approx 0.08
\end{aligned}
$$</li>
<li>Our policy already gives a higher probability to the preferred answer compared to the dispreferred answer, so the dynamic weighting factor becomes:
$$ \sigma (r_\theta (y_l) - r_\theta (y_w)) = \sigma(0.08 - 0.99) \approx 0.287 $$</li>
<li>for first predicted token (can be combined, since both sequences share the same prefix):
$$
\frac{\partial \mathcal{L}_\text{DPO}}{\partial z_1} = 0.287 \left(\begin{pmatrix}
0.5 - 1 \\ 0.1 - 0 \\ 0.4 - 0
\end{pmatrix} - \begin{pmatrix}
0.5 - 0 \\ 0.1 - 0 \\ 0.4 - 1
\end{pmatrix}\right) = \begin{pmatrix} - 0.287 \\ 0 \\ 0.287
\end{pmatrix}
$$</li>
<li>for the second predicted token (given $A$, i.e. preferred answer):
$$\frac{\partial \mathcal{L}_\text{DPO}}{\partial z_2^{(w)}} = 0.287 \begin{pmatrix}
0.3 - 0 \\ 0.6 - 1 \\ 0.1 - 0
\end{pmatrix} = \begin{pmatrix}
0.086 \\ - 0.115 \\ 0.029
\end{pmatrix}$$</li>
<li>for the second predicted token (given $C$, i.e. dispreferred answer):
$$\frac{\partial \mathcal{L}_\text{DPO}}{\partial z_2^{(l)}} = 0.287 \left(-\begin{pmatrix}
0.2 - 0 \\ 0.3 - 1 \\ 0.5 - 0
\end{pmatrix}\right) = \begin{pmatrix} - 0.057 \\ 0.2 \\ - 0.144
\end{pmatrix}$$</li>
</ul>
<p>If we compare those gradients to the gradients of the cross-entropy loss above, we see some similarities, but also some differences:</p>
<ul>
<li>For the first token we have a different gradient distribution. Cross-entropy targets all logits, increasing the likelihood of the ground-truth logit and decreasing all others. DPO only targets the tokens involved in the preferred and dispreferred sequence, due to the same prefix and symmetry mentioned earlier. We can rewrite the gradient calculation as:
$$\frac{\partial \mathcal{L}_\text{DPO}}{\partial z_1} = \sigma (\Delta) \hat{y}_{w_1} - e_{w_1} - (\hat{y}_{l_1} - e_{l_1})$$</li>
<li>Since $\hat{y}_{w_1} - \hat{y}_{l_1}$ we get $(\partial \mathcal{L}_\text{DPO} / (\partial z_1) =  \sigma (\Delta) e_(l_1) - e_(w_1)$. This shows that for shared prefixes, the gradient only depends on the difference between the one-hot vectors of the first diverging tokens.</li>
<li>The gradients for the second token of cross-entropy loss and the preferred sequence in DPO are the same, up to the scaling.</li>
<li>The gradient second token of the dispreferred sequence in DPO is effectively an <em>anti-SFT</em> update. Since the sequence was dispreferred, the model is penalized for choosing token &lsquo;B&rsquo;. The gradient pushes the logit for &lsquo;B&rsquo; down, reducing the probability of this choice in the future when coming from the prefix $[x, C]$</li>
</ul>
<h2 id="summary" class="relative group">Summary <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#summary" aria-label="Anchor">#</a></span></h2><p>To summarize, we saw that the gradients of cross-entropy and DPO have the same core part $\hat{y} - e_y$, but DPO has a crucial <strong>sequence-level weighting factor</strong>, that reduces the gradient for sequences where the preference of the model is already aligned to the expected one.</p>
<blockquote>
<p>This summary is contains my learning notes. They may contain errors or be updated as my understanding evolves. Feel free to report errors or provide feedback to blog@aweers.de</p></blockquote>

      </div>
    </section>
    <footer class="max-w-prose pt-8 print:hidden">
      
  <div class="flex">
    
    
    
    <div class="place-self-center">
      
      
      <div class="text-2xl sm:text-lg">
</div>
    </div>
  </div>


      

      
  


      
    </footer>
  </article>

      </main>
      
        <div
          class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"
          id="to-top"
          hidden="true"
        >
          <a
            href="#the-top"
            class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
            aria-label="Scroll to top"
            title="Scroll to top"
          >
            &uarr;
          </a>
        </div>
      <footer class="py-10 print:hidden">
  
  
  <div class="flex items-center justify-between">
    <div>
      
      
        <p class="text-sm text-neutral-500 dark:text-neutral-400">
            &copy;
            2025
            
        </p>
      
      
      
        <p class="text-xs text-neutral-500 dark:text-neutral-400">
          
          
          Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
            href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href="https://github.com/jpanther/congo" target="_blank" rel="noopener noreferrer">Congo</a>
        </p>
      
    </div>
    <div class="flex flex-row items-center">
      
      
      
      
    </div>
  </div>
  
  
</footer>

    </div><script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ]
        });
    });
</script>
</body>
</html>
